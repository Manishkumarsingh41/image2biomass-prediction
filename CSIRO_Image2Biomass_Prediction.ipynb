{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåæ CSIRO Image2Biomass Prediction - Complete Solution\n",
    "\n",
    "**Author:** Manish Kumar Singh  \n",
    "**Competition:** [CSIRO - Image2Biomass Prediction](https://www.kaggle.com/competitions/csiro-biomass)  \n",
    "**Objective:** Predict pasture biomass from drone and ground images using deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. Introduction & Competition Overview\n",
    "2. Imports & Environment Setup\n",
    "3. Data Loading & Exploration\n",
    "4. Image Visualization\n",
    "5. Data Preprocessing & Augmentation\n",
    "6. Weighted R¬≤ Metric Implementation\n",
    "7. EfficientNetB0 Model Architecture\n",
    "8. Training Configuration & Callbacks\n",
    "9. Model Training\n",
    "10. Training History Visualization\n",
    "11. Validation Evaluation\n",
    "12. Test Predictions & Submission\n",
    "13. Predictions vs Ground Truth Plots\n",
    "14. Final Summary & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Introduction\n",
    "\n",
    "The **CSIRO Image2Biomass Prediction** competition challenges participants to estimate pasture biomass using drone and ground imagery.  \n",
    "Accurate predictions help improve **farm efficiency**, **animal welfare**, and **soil sustainability**.\n",
    "\n",
    "### üéØ Evaluation Metric: Weighted R¬≤\n",
    "The competition uses a weighted version of the R¬≤ (coefficient of determination):\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum w_i(y_i - \\hat{y}_i)^2}{\\sum w_i(y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "### üìä Target Variables & Weights:\n",
    "\n",
    "| Target | Weight |\n",
    "|---------|--------|\n",
    "| Dry_Green_g | 0.1 |\n",
    "| Dry_Dead_g | 0.1 |\n",
    "| Dry_Clover_g | 0.1 |\n",
    "| GDM_g | 0.2 |\n",
    "| Dry_Total_g | 0.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ üì¶ Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import all necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Step 2: Configure warnings and plot style\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Step 3: Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Step 4: Display library versions\n",
    "print(f\"‚úÖ TensorFlow: {tf.__version__}\")\n",
    "print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "print(f\"‚úÖ Pandas: {pd.__version__}\")\n",
    "print(f\"‚úÖ GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ üìä Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define data path\n",
    "DATA_PATH = '/kaggle/input/csiro-biomass'\n",
    "\n",
    "# Step 2: Load CSV files\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "sample_submission = pd.read_csv(f\"{DATA_PATH}/sample_submission.csv\")\n",
    "\n",
    "# Step 3: Define target columns and their weights\n",
    "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "TARGET_WEIGHTS = [0.1, 0.1, 0.1, 0.2, 0.5]\n",
    "\n",
    "# Step 4: Display data information\n",
    "print(\"=\"*60)\n",
    "print(\"üìÅ DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Train shape: {train_df.shape}\")\n",
    "print(f\"üìä Test shape: {test_df.shape}\")\n",
    "print(f\"üìä Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "print(f\"\\nüéØ Target columns: {TARGET_COLS}\")\n",
    "print(f\"‚öñÔ∏è  Target weights: {TARGET_WEIGHTS}\")\n",
    "\n",
    "print(\"\\nüìã First few rows of training data:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nüìä Missing values in target columns:\")\n",
    "print(train_df[TARGET_COLS].isnull().sum())\n",
    "\n",
    "print(\"\\nüìä Target statistics:\")\n",
    "display(train_df[TARGET_COLS].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ üñºÔ∏è Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define function to visualize sample images\n",
    "def visualize_samples(df, img_dir, num=6, show_targets=True):\n",
    "    \"\"\"\n",
    "    Visualize random sample images from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing image information\n",
    "        img_dir: Directory containing images\n",
    "        num: Number of images to display\n",
    "        show_targets: Whether to display target values\n",
    "    \"\"\"\n",
    "    # Sample random images\n",
    "    sample = df.sample(min(num, len(df)), random_state=SEED)\n",
    "    \n",
    "    # Create subplot grid\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (idx, row) in enumerate(sample.iterrows()):\n",
    "        if i >= num:\n",
    "            break\n",
    "            \n",
    "        # Try different image extensions\n",
    "        img_path = None\n",
    "        for ext in ['.jpg', '.jpeg', '.png', '.JPG']:\n",
    "            path = os.path.join(img_dir, f\"{row['image_id']}{ext}\")\n",
    "            if os.path.exists(path):\n",
    "                img_path = path\n",
    "                break\n",
    "        \n",
    "        # Display image if found\n",
    "        if img_path and os.path.exists(img_path):\n",
    "            img = mpimg.imread(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            \n",
    "            # Create title with target information\n",
    "            if show_targets and 'Dry_Total_g' in df.columns:\n",
    "                title = f\"{row['image_id']}\\nTotal Biomass: {row['Dry_Total_g']:.1f}g\"\n",
    "            else:\n",
    "                title = f\"{row['image_id']}\"\n",
    "            \n",
    "            axes[i].set_title(title, fontsize=10, fontweight='bold')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, f\"Image not found:\\n{row['image_id']}\", \n",
    "                        ha='center', va='center')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"üåæ Sample Training Images\", fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Step 2: Visualize training samples\n",
    "print(\"\\nüì∏ Visualizing sample training images...\\n\")\n",
    "visualize_samples(train_df, f\"{DATA_PATH}/train_images\", num=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ ‚öôÔ∏è Data Preprocessing & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define preprocessing parameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# Step 2: Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=VAL_SPLIT, \n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîÑ DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Training samples: {len(train_data)}\")\n",
    "print(f\"üìä Validation samples: {len(val_data)}\")\n",
    "print(f\"üìä Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"üìä Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Step 3: Prepare dataframes with image filenames\n",
    "train_data_prep = train_data.copy()\n",
    "val_data_prep = val_data.copy()\n",
    "\n",
    "# Add .jpg extension to image_id\n",
    "train_data_prep['image_filename'] = train_data_prep['image_id'] + '.jpg'\n",
    "val_data_prep['image_filename'] = val_data_prep['image_id'] + '.jpg'\n",
    "\n",
    "# Step 4: Create ImageDataGenerator with augmentation for training\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Step 5: Create ImageDataGenerator for validation (only rescaling)\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Step 6: Create data flow from dataframes for multi-output regression\n",
    "train_flow = train_gen.flow_from_dataframe(\n",
    "    dataframe=train_data_prep,\n",
    "    directory=f\"{DATA_PATH}/train_images\",\n",
    "    x_col='image_filename',\n",
    "    y_col=TARGET_COLS,  # Multiple target columns\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_mode='raw',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_flow = val_gen.flow_from_dataframe(\n",
    "    dataframe=val_data_prep,\n",
    "    directory=f\"{DATA_PATH}/train_images\",\n",
    "    x_col='image_filename',\n",
    "    y_col=TARGET_COLS,  # Multiple target columns\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_mode='raw',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Data generators created successfully!\")\n",
    "print(f\"‚úÖ Training batches per epoch: {len(train_flow)}\")\n",
    "print(f\"‚úÖ Validation batches per epoch: {len(val_flow)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ üéØ Weighted R¬≤ Metric Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define weighted R¬≤ score function\n",
    "def weighted_r2_score(y_true, y_pred, weights=None):\n",
    "    \"\"\"\n",
    "    Calculate weighted R¬≤ score for multi-output regression.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values (n_samples, n_targets)\n",
    "        y_pred: Predicted values (n_samples, n_targets)\n",
    "        weights: Weights for each target (n_targets,)\n",
    "    \n",
    "    Returns:\n",
    "        Weighted R¬≤ score\n",
    "    \"\"\"\n",
    "    # Set default weights if not provided\n",
    "    if weights is None:\n",
    "        weights = np.ones(y_true.shape[1])\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    # Calculate R¬≤ for each target\n",
    "    r2_scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        y_t = y_true[:, i]\n",
    "        y_p = y_pred[:, i]\n",
    "        \n",
    "        # Calculate sum of squared residuals and total sum of squares\n",
    "        ss_res = np.sum((y_t - y_p) ** 2)\n",
    "        ss_tot = np.sum((y_t - np.mean(y_t)) ** 2)\n",
    "        \n",
    "        # Calculate R¬≤ score\n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    # Calculate weighted average\n",
    "    weighted_r2 = np.sum(np.array(r2_scores) * weights) / np.sum(weights)\n",
    "    return weighted_r2, r2_scores\n",
    "\n",
    "# Step 2: Test the metric with dummy data\n",
    "print(\"=\"*60)\n",
    "print(\"üéØ WEIGHTED R¬≤ METRIC\")\n",
    "print(\"=\"*60)\n",
    "test_true = np.random.rand(100, 5)\n",
    "test_pred = test_true + np.random.rand(100, 5) * 0.1\n",
    "test_score, test_individual = weighted_r2_score(test_true, test_pred, TARGET_WEIGHTS)\n",
    "print(f\"\\n‚úÖ Metric test successful!\")\n",
    "print(f\"\\nüìä Test weighted R¬≤ score: {test_score:.4f}\")\n",
    "print(f\"\\nüìä Individual R¬≤ scores: {[f'{s:.4f}' for s in test_individual]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ üß† EfficientNetB0 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define model building function\n",
    "def build_model(input_shape=(224, 224, 3), num_outputs=5):\n",
    "    \"\"\"\n",
    "    Build EfficientNetB0-based multi-output regression model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image shape\n",
    "        num_outputs: Number of target variables to predict\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load pre-trained EfficientNetB0 without top layers\n",
    "    base = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        input_shape=input_shape,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model for transfer learning\n",
    "    base.trainable = False\n",
    "    \n",
    "    # Build custom top layers\n",
    "    inputs = layers.Input(shape=input_shape, name='input')\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    x = layers.BatchNormalization(name='bn_1')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout_1')(x)\n",
    "    x = layers.Dense(256, activation='relu', name='dense_1')(x)\n",
    "    x = layers.BatchNormalization(name='bn_2')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout_2')(x)\n",
    "    x = layers.Dense(128, activation='relu', name='dense_2')(x)\n",
    "    x = layers.Dropout(0.2, name='dropout_3')(x)\n",
    "    \n",
    "    # Output layer for multiple targets\n",
    "    outputs = layers.Dense(num_outputs, activation='linear', name='output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='EfficientNetB0_Biomass')\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Step 2: Build and display model\n",
    "print(\"=\"*60)\n",
    "print(\"üß† MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "model = build_model(num_outputs=len(TARGET_COLS))\n",
    "print(f\"\\n‚úÖ Model built successfully!\")\n",
    "print(f\"\\nüìä Total parameters: {model.count_params():,}\")\n",
    "print(f\"üìä Output targets: {len(TARGET_COLS)} ({', '.join(TARGET_COLS)})\")\n",
    "print(\"\\nüìã Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ üèãÔ∏è Training Configuration & Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define training parameters\n",
    "EPOCHS = 30\n",
    "\n",
    "# Step 2: Define callbacks for training\n",
    "callbacks_list = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when plateau is reached\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    \n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        'best_biomass_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üèãÔ∏è TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Maximum epochs: {EPOCHS}\")\n",
    "print(f\"üìä Batch size: {BATCH_SIZE}\")\n",
    "print(f\"üìä Learning rate: 1e-3\")\n",
    "print(f\"\\n‚úÖ Callbacks configured:\")\n",
    "print(f\"  - EarlyStopping (patience=7)\")\n",
    "print(f\"  - ReduceLROnPlateau (patience=3, factor=0.5)\")\n",
    "print(f\"  - ModelCheckpoint (save best model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ üöÄ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Start training\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚è∞ Training started...\\n\")\n",
    "\n",
    "# Step 2: Fit the model\n",
    "history = model.fit(\n",
    "    train_flow,\n",
    "    validation_data=val_flow,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"üìä Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"üìä Best validation MAE: {min(history.history['val_mae']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîü üìà Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2, color='#2E86AB')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2, color='#A23B72')\n",
    "axes[0].set_title('üìâ Model Loss (MSE)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2, color='#2E86AB')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2, color='#A23B72')\n",
    "axes[1].set_title('üìä Model MAE', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Training Summary:\")\n",
    "print(f\"  Final Train Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Train MAE: {history.history['mae'][-1]:.4f}\")\n",
    "print(f\"  Final Val MAE: {history.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ üîç Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate predictions on validation set\n",
    "print(\"=\"*60)\n",
    "print(\"üîç VALIDATION EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚è≥ Generating predictions on validation set...\\n\")\n",
    "\n",
    "# Create validation generator (no shuffle)\n",
    "val_flow_eval = val_gen.flow_from_dataframe(\n",
    "    dataframe=val_data_prep,\n",
    "    directory=f\"{DATA_PATH}/train_images\",\n",
    "    x_col='image_filename',\n",
    "    y_col=TARGET_COLS,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_mode='raw',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Step 2: Get predictions\n",
    "val_predictions = model.predict(val_flow_eval, verbose=1)\n",
    "val_true = val_data_prep[TARGET_COLS].values\n",
    "\n",
    "# Step 3: Calculate weighted R¬≤ score\n",
    "weighted_r2, individual_r2 = weighted_r2_score(val_true, val_predictions, TARGET_WEIGHTS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüéØ Weighted R¬≤ Score: {weighted_r2:.4f}\")\n",
    "print(\"\\nüìä Individual Target Performance:\\n\")\n",
    "print(f\"{'Target':<20} {'R¬≤ Score':<12} {'MAE':<12} {'Weight':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, (col, weight) in enumerate(zip(TARGET_COLS, TARGET_WEIGHTS)):\n",
    "    r2 = individual_r2[i]\n",
    "    mae = mean_absolute_error(val_true[:, i], val_predictions[:, i])\n",
    "    print(f\"{col:<20} {r2:<12.4f} {mae:<12.2f} {weight:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ üîÆ Test Predictions & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare test data\n",
    "print(\"=\"*60)\n",
    "print(\"üîÆ GENERATING TEST PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚è≥ Preparing test data...\\n\")\n",
    "\n",
    "test_df_prep = test_df.copy()\n",
    "test_df_prep['image_filename'] = test_df_prep['image_id'] + '.jpg'\n",
    "\n",
    "# Step 2: Create test data generator\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_flow = test_gen.flow_from_dataframe(\n",
    "    dataframe=test_df_prep,\n",
    "    directory=f\"{DATA_PATH}/test_images\",\n",
    "    x_col='image_filename',\n",
    "    y_col=None,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Step 3: Generate predictions\n",
    "print(\"‚è≥ Generating predictions...\\n\")\n",
    "test_predictions = model.predict(test_flow, verbose=1)\n",
    "\n",
    "# Step 4: Create submission dataframe\n",
    "print(\"\\n‚è≥ Creating submission file...\")\n",
    "submission = sample_submission.copy()\n",
    "\n",
    "# Map predictions to submission format\n",
    "for i, col in enumerate(TARGET_COLS):\n",
    "    # Find rows in submission that match this target\n",
    "    target_mask = submission['sample_id'].str.contains(col)\n",
    "    \n",
    "    # Get image IDs for this target\n",
    "    image_ids = submission[target_mask]['sample_id'].str.split('_').str[0]\n",
    "    \n",
    "    # Create mapping from image_id to index\n",
    "    image_id_to_idx = {img_id: idx for idx, img_id in enumerate(test_df['image_id'])}\n",
    "    \n",
    "    # Map predictions to submission\n",
    "    pred_values = [test_predictions[image_id_to_idx[img_id], i] for img_id in image_ids]\n",
    "    submission.loc[target_mask, 'target'] = pred_values\n",
    "\n",
    "# Step 5: Save submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ SUBMISSION FILE CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÑ File: submission.csv\")\n",
    "print(f\"üìä Shape: {submission.shape}\")\n",
    "print(f\"\\nüìã Preview:\")\n",
    "display(submission.head(10))\n",
    "print(f\"\\nüìä Prediction Statistics:\")\n",
    "display(submission['target'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ üìä Predictions vs Ground Truth Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create scatter plots for each target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E']\n",
    "\n",
    "for i, (col, weight, color) in enumerate(zip(TARGET_COLS, TARGET_WEIGHTS, colors)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(val_true[:, i], val_predictions[:, i], alpha=0.6, s=40, color=color, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(val_true[:, i].min(), val_predictions[:, i].min())\n",
    "    max_val = max(val_true[:, i].max(), val_predictions[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2.5, label='Perfect Prediction', alpha=0.8)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = individual_r2[i]\n",
    "    mae = mean_absolute_error(val_true[:, i], val_predictions[:, i])\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('True Values (g)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Predicted Values (g)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{col}\\nR¬≤ = {r2:.4f} | MAE = {mae:.2f}g | Weight = {weight}', \n",
    "                fontsize=13, fontweight='bold', pad=10)\n",
    "    ax.legend(fontsize=10, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('üåæ Validation Set: Predictions vs Ground Truth', fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ üéâ Final Summary & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Display final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*15 + \"üåæ CSIRO IMAGE2BIOMASS PREDICTION - FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä MODEL INFORMATION:\")\n",
    "print(f\"  ‚Ä¢ Architecture: EfficientNetB0 (Transfer Learning)\")\n",
    "print(f\"  ‚Ä¢ Total Parameters: {model.count_params():,}\")\n",
    "print(f\"  ‚Ä¢ Input Size: {IMG_SIZE}x{IMG_SIZE}x3\")\n",
    "print(f\"  ‚Ä¢ Output Targets: {len(TARGET_COLS)}\")\n",
    "\n",
    "print(f\"\\nüìä DATASET INFORMATION:\")\n",
    "print(f\"  ‚Ä¢ Training Samples: {len(train_data)}\")\n",
    "print(f\"  ‚Ä¢ Validation Samples: {len(val_data)}\")\n",
    "print(f\"  ‚Ä¢ Test Samples: {len(test_df)}\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\nüìä TRAINING INFORMATION:\")\n",
    "print(f\"  ‚Ä¢ Epochs Trained: {len(history.history['loss'])}\")\n",
    "print(f\"  ‚Ä¢ Best Val Loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"  ‚Ä¢ Best Val MAE: {min(history.history['val_mae']):.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ VALIDATION PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Weighted R¬≤ Score: {weighted_r2:.4f}\")\n",
    "print(f\"\\n  üìä Individual Target Performance:\")\n",
    "print(f\"  {'-'*66}\")\n",
    "print(f\"  {'Target':<20} {'R¬≤':<12} {'MAE (g)':<15} {'Weight':<10}\")\n",
    "print(f\"  {'-'*66}\")\n",
    "for i, (col, weight) in enumerate(zip(TARGET_COLS, TARGET_WEIGHTS)):\n",
    "    r2 = individual_r2[i]\n",
    "    mae = mean_absolute_error(val_true[:, i], val_predictions[:, i])\n",
    "    print(f\"  {col:<20} {r2:<12.4f} {mae:<15.2f} {weight:<10}\")\n",
    "\n",
    "print(f\"\\nüìÑ SUBMISSION:\")\n",
    "print(f\"  ‚Ä¢ File: submission.csv\")\n",
    "print(f\"  ‚Ä¢ Shape: {submission.shape}\")\n",
    "print(f\"  ‚Ä¢ Status: ‚úÖ Ready for submission\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"üéâ NOTEBOOK EXECUTION COMPLETE! üéâ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"  1. Download 'submission.csv'\")\n",
    "print(f\"  2. Submit to Kaggle competition\")\n",
    "print(f\"  3. Fine-tune hyperparameters for better performance\")\n",
    "print(f\"  4. Try unfreezing some EfficientNetB0 layers for fine-tuning\")\n",
    "print(f\"  5. Experiment with ensemble methods\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
