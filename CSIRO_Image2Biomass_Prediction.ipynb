{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass Prediction - Complete Solution\n",
    "\n",
    "**Author:** Manish Kumar Singh  \n",
    "**Competition:** CSIRO - Image2Biomass Prediction  \n",
    "**Objective:** Predict pasture biomass from images using deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction & Competition Overview\n",
    "2. Imports & Environment Setup\n",
    "3. Data Loading & Exploration\n",
    "4. Data Understanding (Long Format)\n",
    "5. Image Visualization\n",
    "6. Data Preprocessing (Pivot to Wide Format)\n",
    "7. Train/Validation Split\n",
    "8. Data Augmentation & Generators\n",
    "9. Weighted R-Squared Metric\n",
    "10. EfficientNetB0 Model Architecture\n",
    "11. Training Configuration\n",
    "12. Model Training\n",
    "13. Training History Visualization\n",
    "14. Validation Evaluation\n",
    "15. Test Predictions & Submission\n",
    "16. Predictions vs Ground Truth\n",
    "17. Final Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The CSIRO Image2Biomass Prediction competition challenges participants to estimate pasture biomass using drone and ground imagery. This helps improve farm efficiency, animal welfare, and soil sustainability.\n",
    "\n",
    "### Evaluation Metric: Weighted R-Squared\n",
    "\n",
    "The competition uses a weighted version of the R-squared coefficient:\n",
    "\n",
    "### Target Variables & Weights:\n",
    "\n",
    "| Target | Weight |\n",
    "|---------|--------|\n",
    "| Dry_Green_g | 0.1 |\n",
    "| Dry_Dead_g | 0.1 |\n",
    "| Dry_Clover_g | 0.1 |\n",
    "| GDM_g | 0.2 |\n",
    "| Dry_Total_g | 0.5 |\n",
    "\n",
    "**Important:** The train.csv is in LONG format - each image has 5 rows (one per target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import all necessary libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Step 2: Configure warnings and plot style\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Step 3: Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Step 4: Display library versions\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define data path\n",
    "DATA_PATH = '/kaggle/input/csiro-biomass'\n",
    "\n",
    "# Step 2: Load CSV files\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "sample_submission = pd.read_csv(f\"{DATA_PATH}/sample_submission.csv\")\n",
    "\n",
    "# Step 3: Display basic information\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTrain shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "print(\"\\nTrain columns:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 10 rows of training data:\")\n",
    "display(train_df.head(10))\n",
    "\n",
    "print(\"\\nData info:\")\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Understanding (Long Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Understand the long format structure\n",
    "print(\"=\"*60)\n",
    "print(\"DATA STRUCTURE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract image_id from image_path\n",
    "train_df['image_id'] = train_df['image_path'].apply(lambda x: Path(x).stem)\n",
    "test_df['image_id'] = test_df['image_path'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "# Count unique images\n",
    "n_train_images = train_df['image_id'].nunique()\n",
    "n_test_images = test_df['image_id'].nunique()\n",
    "\n",
    "print(f\"\\nUnique training images: {n_train_images}\")\n",
    "print(f\"Unique test images: {n_test_images}\")\n",
    "print(f\"Total training rows: {len(train_df)} (should be {n_train_images} x 5 targets)\")\n",
    "\n",
    "# Show target names\n",
    "print(f\"\\nTarget names:\")\n",
    "print(train_df['target_name'].unique())\n",
    "\n",
    "# Show one image with all its targets\n",
    "print(f\"\\nExample: All targets for one image:\")\n",
    "sample_image = train_df['image_id'].iloc[0]\n",
    "display(train_df[train_df['image_id'] == sample_image][['image_id', 'target_name', 'target']])\n",
    "\n",
    "# Define target columns and weights\n",
    "TARGET_COLS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'GDM_g', 'Dry_Total_g']\n",
    "TARGET_WEIGHTS = {'Dry_Green_g': 0.1, 'Dry_Dead_g': 0.1, 'Dry_Clover_g': 0.1, \n",
    "                  'GDM_g': 0.2, 'Dry_Total_g': 0.5}\n",
    "\n",
    "print(f\"\\nTarget columns: {TARGET_COLS}\")\n",
    "print(f\"Target weights: {TARGET_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Image Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get unique images for visualization\n",
    "unique_images = train_df.drop_duplicates(subset=['image_id'])\n",
    "\n",
    "# Step 2: Sample random images\n",
    "sample_images = unique_images.sample(6, random_state=SEED)\n",
    "\n",
    "# Step 3: Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (idx, row) in enumerate(sample_images.iterrows()):\n",
    "    img_path = f\"{DATA_PATH}/{row['image_path']}\"\n",
    "    \n",
    "    if os.path.exists(img_path):\n",
    "        img = mpimg.imread(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        # Get Dry_Total_g for this image\n",
    "        total_biomass = train_df[(train_df['image_id'] == row['image_id']) & \n",
    "                                 (train_df['target_name'] == 'Dry_Total_g')]['target'].values[0]\n",
    "        \n",
    "        axes[i].set_title(f\"{row['image_id']}\\nTotal Biomass: {total_biomass:.1f}g\", \n",
    "                         fontsize=10, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, f\"Image not found\\n{row['image_id']}\", \n",
    "                    ha='center', va='center')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Sample Training Images\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Data Preprocessing (Pivot to Wide Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Pivot train data from long to wide format\n",
    "# We need one row per image with 5 target columns\n",
    "train_wide = train_df.pivot_table(\n",
    "    index=['image_id', 'image_path'], \n",
    "    columns='target_name', \n",
    "    values='target'\n",
    ").reset_index()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nWide format shape: {train_wide.shape}\")\n",
    "print(f\"Expected: ({n_train_images}, 7) - image_id, image_path, and 5 targets\")\n",
    "\n",
    "print(\"\\nWide format columns:\")\n",
    "print(train_wide.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(train_wide.head())\n",
    "\n",
    "# Step 2: Check for missing values\n",
    "print(\"\\nMissing values in targets:\")\n",
    "print(train_wide[TARGET_COLS].isnull().sum())\n",
    "\n",
    "# Step 3: Statistics\n",
    "print(\"\\nTarget statistics:\")\n",
    "display(train_wide[TARGET_COLS].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split data\n",
    "VAL_SPLIT = 0.2\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    train_wide, \n",
    "    test_size=VAL_SPLIT, \n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAIN/VALIDATION SPLIT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Data Augmentation & Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create data generators with augmentation\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Step 2: Create flow from dataframe\n",
    "train_flow = train_gen.flow_from_dataframe(\n",
    "    dataframe=train_data,\n",
    "    directory=DATA_PATH,\n",
    "    x_col='image_path',\n",
    "    y_col=TARGET_COLS,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_mode='raw',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_flow = val_gen.flow_from_dataframe(\n",
    "    dataframe=val_data,\n",
    "    directory=DATA_PATH,\n",
    "    x_col='image_path',\n",
    "    y_col=TARGET_COLS,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_mode='raw',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=SEED,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA GENERATORS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData generators created successfully\")\n",
    "print(f\"Training batches per epoch: {len(train_flow)}\")\n",
    "print(f\"Validation batches per epoch: {len(val_flow)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Weighted R-Squared Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define weighted R-squared function\n",
    "def weighted_r2_score(y_true, y_pred, target_cols, target_weights):\n",
    "    \"\"\"\n",
    "    Calculate weighted R-squared score.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values (n_samples, n_targets)\n",
    "        y_pred: Predicted values (n_samples, n_targets)\n",
    "        target_cols: List of target column names\n",
    "        target_weights: Dictionary of weights for each target\n",
    "    \n",
    "    Returns:\n",
    "        weighted_r2: Weighted R-squared score\n",
    "        individual_r2: List of individual R-squared scores\n",
    "    \"\"\"\n",
    "    individual_r2 = []\n",
    "    weights = []\n",
    "    \n",
    "    for i, col in enumerate(target_cols):\n",
    "        # Calculate R-squared for this target\n",
    "        r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        individual_r2.append(r2)\n",
    "        weights.append(target_weights[col])\n",
    "    \n",
    "    # Calculate weighted average\n",
    "    weights = np.array(weights)\n",
    "    individual_r2 = np.array(individual_r2)\n",
    "    weighted_r2 = np.sum(individual_r2 * weights) / np.sum(weights)\n",
    "    \n",
    "    return weighted_r2, individual_r2\n",
    "\n",
    "# Step 2: Test the metric\n",
    "print(\"=\"*60)\n",
    "print(\"WEIGHTED R-SQUARED METRIC\")\n",
    "print(\"=\"*60)\n",
    "test_true = np.random.rand(100, 5)\n",
    "test_pred = test_true + np.random.rand(100, 5) * 0.1\n",
    "test_wr2, test_r2 = weighted_r2_score(test_true, test_pred, TARGET_COLS, TARGET_WEIGHTS)\n",
    "print(f\"\\nMetric test successful\")\n",
    "print(f\"Test weighted R-squared: {test_wr2:.4f}\")\n",
    "print(f\"Individual R-squared scores: {[f'{r:.4f}' for r in test_r2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. EfficientNetB0 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build model\n",
    "def build_model(input_shape=(224, 224, 3), num_outputs=5):\n",
    "    \"\"\"\n",
    "    Build EfficientNetB0 model for multi-output regression.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image shape\n",
    "        num_outputs: Number of target outputs\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load pre-trained base model\n",
    "    base = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        input_shape=input_shape,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model\n",
    "    base.trainable = False\n",
    "    \n",
    "    # Build custom top\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_outputs, activation='linear')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Step 2: Create model\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "model = build_model(num_outputs=len(TARGET_COLS))\n",
    "print(f\"\\nModel created successfully\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(f\"\\nModel summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define training parameters\n",
    "EPOCHS = 30\n",
    "\n",
    "# Step 2: Define callbacks\n",
    "callbacks_list = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMaximum epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: 1e-3\")\n",
    "print(f\"\\nCallbacks:\")\n",
    "print(f\"  - EarlyStopping (patience=7)\")\n",
    "print(f\"  - ReduceLROnPlateau (patience=3, factor=0.5)\")\n",
    "print(f\"  - ModelCheckpoint (save best model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train model\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining started...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_flow,\n",
    "    validation_data=val_flow,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nEpochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Best validation MAE: {min(history.history['val_mae']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_title('Model Loss (MSE)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "axes[1].set_title('Model MAE', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate predictions on validation set\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGenerating predictions...\\n\")\n",
    "\n",
    "val_predictions = model.predict(val_flow, verbose=1)\n",
    "val_true = val_data[TARGET_COLS].values\n",
    "\n",
    "# Step 2: Calculate metrics\n",
    "weighted_r2, individual_r2 = weighted_r2_score(val_true, val_predictions, TARGET_COLS, TARGET_WEIGHTS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nWeighted R-Squared Score: {weighted_r2:.4f}\")\n",
    "print(\"\\nIndividual Target Performance:\\n\")\n",
    "print(f\"{'Target':<20} {'R-Squared':<12} {'MAE':<12} {'Weight':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, col in enumerate(TARGET_COLS):\n",
    "    r2 = individual_r2[i]\n",
    "    mae = mean_absolute_error(val_true[:, i], val_predictions[:, i])\n",
    "    weight = TARGET_WEIGHTS[col]\n",
    "    print(f\"{col:<20} {r2:<12.4f} {mae:<12.2f} {weight:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Test Predictions & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare test data\n",
    "print(\"=\"*60)\n",
    "print(\"TEST PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPreparing test data...\\n\")\n",
    "\n",
    "# Get unique test images\n",
    "test_unique = test_df.drop_duplicates(subset=['image_id'])\n",
    "\n",
    "# Create test generator\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_flow = test_gen.flow_from_dataframe(\n",
    "    dataframe=test_unique,\n",
    "    directory=DATA_PATH,\n",
    "    x_col='image_path',\n",
    "    y_col=None,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_mode=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Step 2: Generate predictions\n",
    "print(\"Generating predictions...\\n\")\n",
    "test_predictions = model.predict(test_flow, verbose=1)\n",
    "\n",
    "# Step 3: Create submission\n",
    "print(\"\\nCreating submission file...\")\n",
    "\n",
    "# Create a mapping from image_id to predictions\n",
    "image_to_pred = {}\n",
    "for idx, img_id in enumerate(test_unique['image_id']):\n",
    "    image_to_pred[img_id] = test_predictions[idx]\n",
    "\n",
    "# Fill submission\n",
    "submission = sample_submission.copy()\n",
    "for idx, row in submission.iterrows():\n",
    "    sample_id = row['sample_id']\n",
    "    img_id, target_name = sample_id.split('__')\n",
    "    \n",
    "    if img_id in image_to_pred:\n",
    "        target_idx = TARGET_COLS.index(target_name)\n",
    "        submission.loc[idx, 'target'] = image_to_pred[img_id][target_idx]\n",
    "\n",
    "# Step 4: Save submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFile: submission.csv\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "print(f\"\\nPreview:\")\n",
    "display(submission.head(10))\n",
    "print(f\"\\nStatistics:\")\n",
    "display(submission['target'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16. Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create scatter plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(TARGET_COLS):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(val_true[:, i], val_predictions[:, i], alpha=0.6, s=40)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(val_true[:, i].min(), val_predictions[:, i].min())\n",
    "    max_val = max(val_true[:, i].max(), val_predictions[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Metrics\n",
    "    r2 = individual_r2[i]\n",
    "    mae = mean_absolute_error(val_true[:, i], val_predictions[:, i])\n",
    "    \n",
    "    ax.set_xlabel('True Values (g)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Predicted Values (g)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{col}\\nR-Squared = {r2:.4f}, MAE = {mae:.2f}g', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Validation: Predictions vs Ground Truth', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 17. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Display final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \" * 15 + \"CSIRO IMAGE2BIOMASS PREDICTION - FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nMODEL INFORMATION:\")\n",
    "print(f\"  Architecture: EfficientNetB0 (Transfer Learning)\")\n",
    "print(f\"  Total Parameters: {model.count_params():,}\")\n",
    "print(f\"  Input Size: {IMG_SIZE}x{IMG_SIZE}x3\")\n",
    "print(f\"  Output Targets: {len(TARGET_COLS)}\")\n",
    "\n",
    "print(f\"\\nDATASET INFORMATION:\")\n",
    "print(f\"  Training Images: {len(train_data)}\")\n",
    "print(f\"  Validation Images: {len(val_data)}\")\n",
    "print(f\"  Test Images: {len(test_unique)}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\nTRAINING INFORMATION:\")\n",
    "print(f\"  Epochs Trained: {len(history.history['loss'])}\")\n",
    "print(f\"  Best Val Loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"  Best Val MAE: {min(history.history['val_mae']):.4f}\")\n",
    "\n",
    "print(f\"\\nVALIDATION PERFORMANCE:\")\n",
    "print(f\"  Weighted R-Squared Score: {weighted_r2:.4f}\")\n",
    "print(f\"\\n  Individual Target Performance:\")\n",
    "print(f\"  {'-'*66}\")\n",
    "print(f\"  {'Target':<20} {'R-Squared':<12} {'MAE (g)':<15} {'Weight':<10}\")\n",
    "print(f\"  {'-'*66}\")\n",
    "for i, col in enumerate(TARGET_COLS):\n",
    "    r2 = individual_r2[i]\n",
    "    mae = mean_absolute_error(val_true[:, i], val_predictions[:, i])\n",
    "    weight = TARGET_WEIGHTS[col]\n",
    "    print(f\"  {col:<20} {r2:<12.4f} {mae:<15.2f} {weight:<10}\")\n",
    "\n",
    "print(f\"\\nSUBMISSION:\")\n",
    "print(f\"  File: submission.csv\")\n",
    "print(f\"  Shape: {submission.shape}\")\n",
    "print(f\"  Status: Ready for submission\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\" \" * 20 + \"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Download submission.csv\")\n",
    "print(f\"  2. Submit to Kaggle competition\")\n",
    "print(f\"  3. Try fine-tuning hyperparameters\")\n",
    "print(f\"  4. Experiment with different architectures\")\n",
    "print(f\"  5. Consider ensemble methods\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
