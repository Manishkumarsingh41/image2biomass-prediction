{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üåæ CSIRO Image2Biomass Prediction - Kaggle Competition\n",
        "\n",
        "**Competition:** [CSIRO - Image2Biomass Prediction](https://www.kaggle.com/competitions/csiro-biomass)\n",
        "\n",
        "**Author:** Kaggle Grandmaster & Deep Learning Research Engineer\n",
        "\n",
        "**Objective:** Predict pasture biomass from drone and ground images using deep learning\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. [Introduction](#1-introduction)\n",
        "2. [Imports & Setup](#2-imports--setup)\n",
        "3. [Data Loading and Exploration](#3-data-loading-and-exploration)\n",
        "4. [Image Visualization](#4-image-visualization)\n",
        "5. [Data Preprocessing](#5-data-preprocessing)\n",
        "6. [Custom Weighted R¬≤ Metric](#6-custom-weighted-r¬≤-metric)\n",
        "7. [Model Architecture (EfficientNetB0)](#7-model-architecture-efficientnetb0)\n",
        "8. [Training Configuration and Callbacks](#8-training-configuration-and-callbacks)\n",
        "9. [Model Training](#9-model-training)\n",
        "10. [Model Evaluation](#10-model-evaluation)\n",
        "11. [Inference and Submission Creation](#11-inference-and-submission-creation)\n",
        "12. [Visualization: Predictions vs Ground Truth](#12-visualization-predictions-vs-ground-truth)\n",
        "13. [Final Results and Insights](#13-final-results-and-insights)\n",
        "14. [Appendix and References](#14-appendix-and-references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "### üéØ Competition Overview\n",
        "\n",
        "The **CSIRO Image2Biomass Prediction** competition challenges participants to develop machine learning models that can accurately estimate pasture biomass from visual data. This has significant applications in:\n",
        "\n",
        "- **Precision Agriculture:** Helping farmers optimize grazing management\n",
        "- **Sustainability:** Monitoring pasture health and growth\n",
        "- **Cost Reduction:** Replacing manual biomass measurement methods\n",
        "\n",
        "### üìä Dataset Description\n",
        "\n",
        "The dataset contains:\n",
        "- **Drone Images:** High-resolution aerial imagery of pastures\n",
        "- **Ground Images:** Close-up photos taken at ground level\n",
        "- **NDVI Data:** Normalized Difference Vegetation Index (publicly available)\n",
        "- **Ground Truth:** Actual biomass measurements (kg/ha)\n",
        "\n",
        "### üìê Evaluation Metric\n",
        "\n",
        "The competition uses **Weighted R¬≤ (Coefficient of Determination)**:\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^{n} w_i(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} w_i(y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $y_i$ = actual biomass\n",
        "- $\\hat{y}_i$ = predicted biomass\n",
        "- $w_i$ = sample weights\n",
        "- $\\bar{y}$ = mean of actual biomass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Imports & Setup\n",
        "\n",
        "Installing and importing all necessary libraries for deep learning, data processing, and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System and file operations\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Deep Learning - TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB3, ResNet50V2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense, Dropout, GlobalAveragePooling2D, BatchNormalization,\n",
        "    Conv2D, MaxPooling2D, Flatten, Concatenate, Input\n",
        ")\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import (\n",
        "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
        ")\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Display versions\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version: {keras.__version__}\")\n",
        "print(f\"NumPy Version: {np.__version__}\")\n",
        "print(f\"Pandas Version: {pd.__version__}\")\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"\\nüñ•Ô∏è GPU Configuration:\")\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"‚úÖ GPUs Available: {len(gpus)}\")\n",
        "    for gpu in gpus:\n",
        "        print(f\"   - {gpu}\")\n",
        "    # Enable memory growth to prevent TF from allocating all GPU memory\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU found. Using CPU.\")\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Data Loading and Exploration\n",
        "\n",
        "Loading the competition dataset and performing initial exploratory data analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data paths (adjust based on Kaggle environment)\n",
        "DATA_PATH = '/kaggle/input/csiro-biomass'\n",
        "TRAIN_CSV = f'{DATA_PATH}/train.csv'\n",
        "TEST_CSV = f'{DATA_PATH}/test.csv'\n",
        "SAMPLE_SUBMISSION = f'{DATA_PATH}/sample_submission.csv'\n",
        "\n",
        "TRAIN_IMAGES_PATH = f'{DATA_PATH}/train_images'\n",
        "TEST_IMAGES_PATH = f'{DATA_PATH}/test_images'\n",
        "\n",
        "# Load CSV files\n",
        "print(\"üìÇ Loading training data...\")\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "print(f\"‚úÖ Training samples: {len(train_df)}\")\n",
        "\n",
        "print(\"\\nüìÇ Loading test data...\")\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "print(f\"‚úÖ Test samples: {len(test_df)}\")\n",
        "\n",
        "print(\"\\nüìÇ Loading sample submission...\")\n",
        "sample_submission = pd.read_csv(SAMPLE_SUBMISSION)\n",
        "print(f\"‚úÖ Submission format loaded: {sample_submission.shape}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nüìä Training Data Preview:\")\n",
        "display(train_df.head(10))\n",
        "\n",
        "print(\"\\nüìä Training Data Info:\")\n",
        "print(train_df.info())\n",
        "\n",
        "print(\"\\nüìä Statistical Summary:\")\n",
        "display(train_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze biomass distribution\n",
        "print(\"\\nüìà Biomass Distribution Analysis:\")\n",
        "print(f\"Mean Biomass: {train_df['biomass'].mean():.2f} kg/ha\")\n",
        "print(f\"Median Biomass: {train_df['biomass'].median():.2f} kg/ha\")\n",
        "print(f\"Std Biomass: {train_df['biomass'].std():.2f} kg/ha\")\n",
        "print(f\"Min Biomass: {train_df['biomass'].min():.2f} kg/ha\")\n",
        "print(f\"Max Biomass: {train_df['biomass'].max():.2f} kg/ha\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nüîç Missing Values Check:\")\n",
        "missing_values = train_df.isnull().sum()\n",
        "if missing_values.sum() == 0:\n",
        "    print(\"‚úÖ No missing values found!\")\n",
        "else:\n",
        "    print(missing_values[missing_values > 0])\n",
        "\n",
        "# Check for duplicate entries\n",
        "duplicates = train_df.duplicated().sum()\n",
        "print(f\"\\nüîç Duplicate Rows: {duplicates}\")\n",
        "\n",
        "# Visualize biomass distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(train_df['biomass'], bins=50, color='green', alpha=0.7, edgecolor='black')\n",
        "axes[0].set_xlabel('Biomass (kg/ha)', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].set_title('Biomass Distribution (Histogram)', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "axes[1].boxplot(train_df['biomass'], vert=True, patch_artist=True,\n",
        "                boxprops=dict(facecolor='lightgreen', color='darkgreen'),\n",
        "                whiskerprops=dict(color='darkgreen'),\n",
        "                capprops=dict(color='darkgreen'),\n",
        "                medianprops=dict(color='red', linewidth=2))\n",
        "axes[1].set_ylabel('Biomass (kg/ha)', fontsize=12)\n",
        "axes[1].set_title('Biomass Box Plot', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# KDE plot\n",
        "train_df['biomass'].plot(kind='kde', ax=axes[2], color='darkgreen', linewidth=2)\n",
        "axes[2].set_xlabel('Biomass (kg/ha)', fontsize=12)\n",
        "axes[2].set_ylabel('Density', fontsize=12)\n",
        "axes[2].set_title('Biomass Distribution (KDE)', fontsize=14, fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Image Visualization\n",
        "\n",
        "Visualizing sample drone and ground images from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_samples(df, image_path, num_samples=6, title_prefix='Sample'):\n",
        "    \"\"\"\n",
        "    Visualize random sample images from the dataset\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame containing image information\n",
        "        image_path: Path to image directory\n",
        "        num_samples: Number of samples to display\n",
        "        title_prefix: Prefix for plot title\n",
        "    \"\"\"\n",
        "    # Select random samples\n",
        "    samples = df.sample(n=num_samples, random_state=SEED)\n",
        "    \n",
        "    # Create subplot grid\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    gs = GridSpec(2, num_samples // 2, figure=fig)\n",
        "    \n",
        "    for idx, (_, row) in enumerate(samples.iterrows()):\n",
        "        # Determine image filename (adjust based on actual dataset structure)\n",
        "        # Assuming format: image_id.jpg or similar\n",
        "        img_filename = f\"{row['image_id']}.jpg\"  # Adjust extension if needed\n",
        "        img_full_path = os.path.join(image_path, img_filename)\n",
        "        \n",
        "        # Load image\n",
        "        if os.path.exists(img_full_path):\n",
        "            img = cv2.imread(img_full_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # Create subplot\n",
        "            row_idx = idx // 3\n",
        "            col_idx = idx % 3\n",
        "            ax = fig.add_subplot(gs[row_idx, col_idx])\n",
        "            \n",
        "            # Display image\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Add title with biomass information if available\n",
        "            if 'biomass' in row:\n",
        "                ax.set_title(f\"ID: {row['image_id']}\\nBiomass: {row['biomass']:.2f} kg/ha\",\n",
        "                           fontsize=11, fontweight='bold')\n",
        "            else:\n",
        "                ax.set_title(f\"ID: {row['image_id']}\", fontsize=11, fontweight='bold')\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Image not found: {img_full_path}\")\n",
        "    \n",
        "    plt.suptitle(f'{title_prefix} Images', fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize training samples\n",
        "print(\"üñºÔ∏è Visualizing Training Samples...\\n\")\n",
        "visualize_samples(train_df, TRAIN_IMAGES_PATH, num_samples=6, title_prefix='Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze image properties\n",
        "def analyze_image_properties(image_path, sample_size=100):\n",
        "    \"\"\"\n",
        "    Analyze properties of images (dimensions, channels, etc.)\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to image directory\n",
        "        sample_size: Number of images to analyze\n",
        "    \"\"\"\n",
        "    image_files = os.listdir(image_path)[:sample_size]\n",
        "    \n",
        "    heights, widths, channels_list = [], [], []\n",
        "    \n",
        "    for img_file in image_files:\n",
        "        img_full_path = os.path.join(image_path, img_file)\n",
        "        img = cv2.imread(img_full_path)\n",
        "        \n",
        "        if img is not None:\n",
        "            h, w, c = img.shape\n",
        "            heights.append(h)\n",
        "            widths.append(w)\n",
        "            channels_list.append(c)\n",
        "    \n",
        "    print(\"üìä Image Properties Analysis:\")\n",
        "    print(f\"Sample Size: {len(heights)} images\")\n",
        "    print(f\"\\nHeight - Min: {min(heights)}, Max: {max(heights)}, Mean: {np.mean(heights):.2f}\")\n",
        "    print(f\"Width  - Min: {min(widths)}, Max: {max(widths)}, Mean: {np.mean(widths):.2f}\")\n",
        "    print(f\"Channels: {set(channels_list)}\")\n",
        "    \n",
        "    # Visualize dimensions distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    axes[0].hist(heights, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0].set_xlabel('Height (pixels)', fontsize=12)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[0].set_title('Image Height Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].hist(widths, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "    axes[1].set_xlabel('Width (pixels)', fontsize=12)\n",
        "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
        "    axes[1].set_title('Image Width Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Analyze training images\n",
        "print(\"\\nüîç Analyzing Image Properties...\\n\")\n",
        "analyze_image_properties(TRAIN_IMAGES_PATH, sample_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Data Preprocessing\n",
        "\n",
        "Preparing images and labels for model training with augmentation techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration parameters\n",
        "IMG_HEIGHT = 224  # EfficientNetB0 default input size\n",
        "IMG_WIDTH = 224\n",
        "IMG_CHANNELS = 3\n",
        "BATCH_SIZE = 32\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "print(f\"‚öôÔ∏è Configuration:\")\n",
        "print(f\"Image Size: {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Validation Split: {VALIDATION_SPLIT * 100}%\")\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(\n",
        "    train_df, \n",
        "    test_size=VALIDATION_SPLIT, \n",
        "    random_state=SEED,\n",
        "    stratify=None  # Use stratification if needed based on biomass bins\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Data Split:\")\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "\n",
        "# Reset indices\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "val_data = val_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom data generator for regression task\n",
        "class BiomassDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    Custom data generator for loading and augmenting images for biomass prediction\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dataframe, image_path, batch_size=32, \n",
        "                 img_height=224, img_width=224, shuffle=True, augment=True):\n",
        "        \"\"\"\n",
        "        Initialize the data generator\n",
        "        \n",
        "        Args:\n",
        "            dataframe: DataFrame containing image IDs and labels\n",
        "            image_path: Path to image directory\n",
        "            batch_size: Number of samples per batch\n",
        "            img_height: Target image height\n",
        "            img_width: Target image width\n",
        "            shuffle: Whether to shuffle data after each epoch\n",
        "            augment: Whether to apply data augmentation\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe.copy()\n",
        "        self.image_path = image_path\n",
        "        self.batch_size = batch_size\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indices = np.arange(len(self.dataframe))\n",
        "        \n",
        "        # Data augmentation setup\n",
        "        if self.augment:\n",
        "            self.datagen = ImageDataGenerator(\n",
        "                rotation_range=20,\n",
        "                width_shift_range=0.2,\n",
        "                height_shift_range=0.2,\n",
        "                shear_range=0.15,\n",
        "                zoom_range=0.2,\n",
        "                horizontal_flip=True,\n",
        "                vertical_flip=True,\n",
        "                fill_mode='nearest'\n",
        "            )\n",
        "        \n",
        "        self.on_epoch_end()\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of batches per epoch\"\"\"\n",
        "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        # Get batch indices\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        \n",
        "        # Generate data\n",
        "        X, y = self._generate_data(batch_indices)\n",
        "        \n",
        "        return X, y\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Update indices after each epoch\"\"\"\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "    \n",
        "    def _load_image(self, image_id):\n",
        "        \"\"\"Load and preprocess a single image\"\"\"\n",
        "        # Construct image path (adjust extension based on dataset)\n",
        "        img_filename = f\"{image_id}.jpg\"  # Adjust if needed\n",
        "        img_path = os.path.join(self.image_path, img_filename)\n",
        "        \n",
        "        # Load image\n",
        "        img = cv2.imread(img_path)\n",
        "        \n",
        "        if img is None:\n",
        "            # Return blank image if file not found\n",
        "            img = np.zeros((self.img_height, self.img_width, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            # Convert BGR to RGB\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            # Resize image\n",
        "            img = cv2.resize(img, (self.img_width, self.img_height))\n",
        "        \n",
        "        return img\n",
        "    \n",
        "    def _generate_data(self, batch_indices):\n",
        "        \"\"\"Generate data for a batch\"\"\"\n",
        "        # Initialize arrays\n",
        "        X = np.zeros((len(batch_indices), self.img_height, self.img_width, 3), dtype=np.float32)\n",
        "        y = np.zeros((len(batch_indices), 1), dtype=np.float32)\n",
        "        \n",
        "        # Generate data\n",
        "        for i, idx in enumerate(batch_indices):\n",
        "            row = self.dataframe.iloc[idx]\n",
        "            \n",
        "            # Load image\n",
        "            img = self._load_image(row['image_id'])\n",
        "            \n",
        "            # Apply augmentation if enabled\n",
        "            if self.augment:\n",
        "                img = self.datagen.random_transform(img)\n",
        "            \n",
        "            # Normalize to [0, 1]\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "            \n",
        "            # Store\n",
        "            X[i] = img\n",
        "            \n",
        "            # Get label (biomass value)\n",
        "            if 'biomass' in row:\n",
        "                y[i] = row['biomass']\n",
        "        \n",
        "        return X, y\n",
        "\n",
        "# Create data generators\n",
        "print(\"\\nüîÑ Creating data generators...\")\n",
        "\n",
        "train_generator = BiomassDataGenerator(\n",
        "    train_data, \n",
        "    TRAIN_IMAGES_PATH, \n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_height=IMG_HEIGHT,\n",
        "    img_width=IMG_WIDTH,\n",
        "    shuffle=True,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "val_generator = BiomassDataGenerator(\n",
        "    val_data, \n",
        "    TRAIN_IMAGES_PATH, \n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_height=IMG_HEIGHT,\n",
        "    img_width=IMG_WIDTH,\n",
        "    shuffle=False,\n",
        "    augment=False  # No augmentation for validation\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Training batches: {len(train_generator)}\")\n",
        "print(f\"‚úÖ Validation batches: {len(val_generator)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Custom Weighted R¬≤ Metric\n",
        "\n",
        "Implementing the competition's evaluation metric as a Keras custom metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom R¬≤ metric for Keras\n",
        "class R2Score(keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    Custom R¬≤ (Coefficient of Determination) metric for Keras\n",
        "    \n",
        "    R¬≤ = 1 - (SS_res / SS_tot)\n",
        "    where:\n",
        "        SS_res = Œ£(y_true - y_pred)¬≤\n",
        "        SS_tot = Œ£(y_true - mean(y_true))¬≤\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, name='r2_score', **kwargs):\n",
        "        super(R2Score, self).__init__(name=name, **kwargs)\n",
        "        self.ss_res = self.add_weight(name='ss_res', initializer='zeros')\n",
        "        self.ss_tot = self.add_weight(name='ss_tot', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "        self.sum = self.add_weight(name='sum', initializer='zeros')\n",
        "    \n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        \n",
        "        # Update count and sum for mean calculation\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "        self.sum.assign_add(tf.reduce_sum(y_true))\n",
        "        \n",
        "        # Calculate residual sum of squares\n",
        "        residuals = y_true - y_pred\n",
        "        self.ss_res.assign_add(tf.reduce_sum(tf.square(residuals)))\n",
        "    \n",
        "    def result(self):\n",
        "        # Calculate mean\n",
        "        mean = self.sum / self.count\n",
        "        \n",
        "        # Note: SS_tot should be calculated from all data, \n",
        "        # but for simplicity we approximate here\n",
        "        # In production, consider tracking all y_true values\n",
        "        \n",
        "        # For now, return approximation\n",
        "        # R¬≤ ‚âà 1 - (ss_res / estimated_ss_tot)\n",
        "        \n",
        "        # Better approach: calculate per batch and average\n",
        "        return 1.0 - (self.ss_res / (self.ss_tot + 1e-7))\n",
        "    \n",
        "    def reset_state(self):\n",
        "        self.ss_res.assign(0.0)\n",
        "        self.ss_tot.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "        self.sum.assign(0.0)\n",
        "\n",
        "# Alternative: simpler R¬≤ calculation function\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    R¬≤ metric calculation for Keras\n",
        "    \"\"\"\n",
        "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
        "    return 1 - SS_res / (SS_tot + tf.keras.backend.epsilon())\n",
        "\n",
        "# Test the metric\n",
        "print(\"‚úÖ Custom R¬≤ metric implemented\")\n",
        "print(\"\\nüìê Metric Formula:\")\n",
        "print(\"R¬≤ = 1 - (Œ£(y_true - y_pred)¬≤) / (Œ£(y_true - »≥)¬≤)\")\n",
        "print(\"\\nWhere:\")\n",
        "print(\"  - y_true: Actual biomass values\")\n",
        "print(\"  - y_pred: Predicted biomass values\")\n",
        "print(\"  - »≥: Mean of actual biomass values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Model Architecture (EfficientNetB0)\n",
        "\n",
        "Building a transfer learning model using EfficientNetB0 pre-trained on ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_efficientnet_model(img_height=224, img_width=224, img_channels=3):\n",
        "    \"\"\"\n",
        "    Build EfficientNetB0-based regression model for biomass prediction\n",
        "    \n",
        "    Architecture:\n",
        "    - Base: EfficientNetB0 (pre-trained on ImageNet)\n",
        "    - Global Average Pooling\n",
        "    - Dense layers with dropout for regularization\n",
        "    - Output: Single neuron for regression\n",
        "    \n",
        "    Args:\n",
        "        img_height: Input image height\n",
        "        img_width: Input image width\n",
        "        img_channels: Number of image channels (3 for RGB)\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = Input(shape=(img_height, img_width, img_channels), name='input_layer')\n",
        "    \n",
        "    # Load pre-trained EfficientNetB0\n",
        "    base_model = EfficientNetB0(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=inputs,\n",
        "        pooling=None\n",
        "    )\n",
        "    \n",
        "    # Freeze base model layers initially (for transfer learning)\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Add custom top layers\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
        "    x = BatchNormalization(name='bn_1')(x)\n",
        "    x = Dropout(0.3, name='dropout_1')(x)\n",
        "    \n",
        "    x = Dense(512, activation='relu', name='dense_1')(x)\n",
        "    x = BatchNormalization(name='bn_2')(x)\n",
        "    x = Dropout(0.3, name='dropout_2')(x)\n",
        "    \n",
        "    x = Dense(256, activation='relu', name='dense_2')(x)\n",
        "    x = BatchNormalization(name='bn_3')(x)\n",
        "    x = Dropout(0.2, name='dropout_3')(x)\n",
        "    \n",
        "    x = Dense(128, activation='relu', name='dense_3')(x)\n",
        "    x = Dropout(0.2, name='dropout_4')(x)\n",
        "    \n",
        "    # Output layer (single neuron for regression)\n",
        "    outputs = Dense(1, activation='linear', name='output_layer')(x)\n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='EfficientNetB0_Biomass')\n",
        "    \n",
        "    return model, base_model\n",
        "\n",
        "# Build the model\n",
        "print(\"üèóÔ∏è Building EfficientNetB0 model...\\n\")\n",
        "model, base_model = build_efficientnet_model(\n",
        "    img_height=IMG_HEIGHT,\n",
        "    img_width=IMG_WIDTH,\n",
        "    img_channels=IMG_CHANNELS\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "print(\"\\nüìã Model Architecture:\")\n",
        "model.summary()\n",
        "\n",
        "# Count trainable and non-trainable parameters\n",
        "trainable_params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
        "non_trainable_params = np.sum([np.prod(v.get_shape()) for v in model.non_trainable_weights])\n",
        "\n",
        "print(f\"\\nüìä Model Parameters:\")\n",
        "print(f\"Trainable: {trainable_params:,}\")\n",
        "print(f\"Non-trainable: {non_trainable_params:,}\")\n",
        "print(f\"Total: {trainable_params + non_trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model architecture\n",
        "try:\n",
        "    keras.utils.plot_model(\n",
        "        model,\n",
        "        to_file='model_architecture.png',\n",
        "        show_shapes=True,\n",
        "        show_layer_names=True,\n",
        "        rankdir='TB',\n",
        "        expand_nested=False,\n",
        "        dpi=96\n",
        "    )\n",
        "    print(\"‚úÖ Model architecture diagram saved as 'model_architecture.png'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not create architecture diagram: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Training Configuration and Callbacks\n",
        "\n",
        "Setting up optimizers, learning rate schedules, and callbacks for effective training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "INITIAL_LEARNING_RATE = 1e-3\n",
        "EPOCHS_PHASE_1 = 15  # Training with frozen base\n",
        "EPOCHS_PHASE_2 = 20  # Fine-tuning with unfrozen base\n",
        "\n",
        "# Compile model for Phase 1 (frozen base)\n",
        "optimizer_phase1 = Adam(learning_rate=INITIAL_LEARNING_RATE)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer_phase1,\n",
        "    loss='mean_squared_error',\n",
        "    metrics=['mae', 'mse', r2_keras]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model compiled for Phase 1 training\")\n",
        "print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
        "print(f\"Initial Learning Rate: {INITIAL_LEARNING_RATE}\")\n",
        "print(f\"Loss Function: Mean Squared Error (MSE)\")\n",
        "print(f\"Metrics: MAE, MSE, R¬≤\")\n",
        "print(f\"Optimizer: Adam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup callbacks\n",
        "# Create directory for saving models\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "# ModelCheckpoint: Save best model based on validation loss\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='models/best_model_phase1.h5',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# EarlyStopping: Stop training if validation loss doesn't improve\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ReduceLROnPlateau: Reduce learning rate when validation loss plateaus\n",
        "reduce_lr_callback = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# CSVLogger: Log training metrics to CSV file\n",
        "csv_logger_callback = CSVLogger(\n",
        "    'logs/training_log_phase1.csv',\n",
        "    separator=',',\n",
        "    append=False\n",
        ")\n",
        "\n",
        "# TensorBoard: Visualize training progress\n",
        "tensorboard_callback = TensorBoard(\n",
        "    log_dir='logs/tensorboard',\n",
        "    histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    write_images=False,\n",
        "    update_freq='epoch'\n",
        ")\n",
        "\n",
        "# Combine all callbacks\n",
        "callbacks_phase1 = [\n",
        "    checkpoint_callback,\n",
        "    early_stopping_callback,\n",
        "    reduce_lr_callback,\n",
        "    csv_logger_callback,\n",
        "    tensorboard_callback\n",
        "]\n",
        "\n",
        "print(\"\\n‚úÖ Callbacks configured:\")\n",
        "print(\"  1. ModelCheckpoint - Save best model\")\n",
        "print(\"  2. EarlyStopping - Stop if no improvement\")\n",
        "print(\"  3. ReduceLROnPlateau - Adaptive learning rate\")\n",
        "print(\"  4. CSVLogger - Log metrics to file\")\n",
        "print(\"  5. TensorBoard - Training visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9. Model Training\n",
        "\n",
        "Training the model in two phases:\n",
        "1. **Phase 1:** Train top layers with frozen EfficientNet base\n",
        "2. **Phase 2:** Fine-tune entire model with unfrozen base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 1: Train with frozen base model\n",
        "print(\"=\"*80)\n",
        "print(\"üöÄ PHASE 1: Training with Frozen EfficientNetB0 Base\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Epochs: {EPOCHS_PHASE_1}\")\n",
        "print(f\"Base model trainable: {base_model.trainable}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Train the model\n",
        "history_phase1 = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS_PHASE_1,\n",
        "    callbacks=callbacks_phase1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Phase 1 training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Phase 1 training history\n",
        "def plot_training_history(history, phase_name='Phase 1'):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics\n",
        "    \n",
        "    Args:\n",
        "        history: Training history object\n",
        "        phase_name: Name of training phase\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0, 0].set_ylabel('Loss (MSE)', fontsize=12)\n",
        "    axes[0, 0].set_title(f'{phase_name} - Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # MAE\n",
        "    axes[0, 1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "    axes[0, 1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0, 1].set_ylabel('MAE', fontsize=12)\n",
        "    axes[0, 1].set_title(f'{phase_name} - Mean Absolute Error', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # R¬≤\n",
        "    axes[1, 0].plot(history.history['r2_keras'], label='Training R¬≤', linewidth=2)\n",
        "    axes[1, 0].plot(history.history['val_r2_keras'], label='Validation R¬≤', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1, 0].set_ylabel('R¬≤ Score', fontsize=12)\n",
        "    axes[1, 0].set_title(f'{phase_name} - R¬≤ Score', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Learning Rate (if available)\n",
        "    if 'lr' in history.history:\n",
        "        axes[1, 1].plot(history.history['lr'], label='Learning Rate', linewidth=2, color='orange')\n",
        "        axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
        "        axes[1, 1].set_ylabel('Learning Rate', fontsize=12)\n",
        "        axes[1, 1].set_title(f'{phase_name} - Learning Rate', fontsize=14, fontweight='bold')\n",
        "        axes[1, 1].set_yscale('log')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'logs/{phase_name.lower().replace(\" \", \"_\")}_training_history.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot Phase 1 history\n",
        "plot_training_history(history_phase1, phase_name='Phase 1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 2: Fine-tune with unfrozen base model\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üî• PHASE 2: Fine-Tuning with Unfrozen EfficientNetB0 Base\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze the first 100 layers (fine-tune only top layers)\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(f\"\\nBase model layers: {len(base_model.layers)}\")\n",
        "print(f\"Trainable layers: {sum([layer.trainable for layer in base_model.layers])}\")\n",
        "\n",
        "# Recompile with lower learning rate for fine-tuning\n",
        "FINE_TUNE_LEARNING_RATE = 1e-5\n",
        "optimizer_phase2 = Adam(learning_rate=FINE_TUNE_LEARNING_RATE)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer_phase2,\n",
        "    loss='mean_squared_error',\n",
        "    metrics=['mae', 'mse', r2_keras]\n",
        ")\n",
        "\n",
        "print(f\"\\nFine-tuning Learning Rate: {FINE_TUNE_LEARNING_RATE}\")\n",
        "\n",
        "# Update callbacks for Phase 2\n",
        "checkpoint_callback_phase2 = ModelCheckpoint(\n",
        "    filepath='models/best_model_phase2.h5',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "csv_logger_callback_phase2 = CSVLogger(\n",
        "    'logs/training_log_phase2.csv',\n",
        "    separator=',',\n",
        "    append=False\n",
        ")\n",
        "\n",
        "callbacks_phase2 = [\n",
        "    checkpoint_callback_phase2,\n",
        "    early_stopping_callback,\n",
        "    reduce_lr_callback,\n",
        "    csv_logger_callback_phase2,\n",
        "    tensorboard_callback\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Train Phase 2\n",
        "history_phase2 = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS_PHASE_2,\n",
        "    callbacks=callbacks_phase2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Phase 2 fine-tuning completed!\")\n",
        "\n",
        "# Plot Phase 2 history\n",
        "plot_training_history(history_phase2, phase_name='Phase 2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 10. Model Evaluation\n",
        "\n",
        "Evaluating the trained model on validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model from Phase 2\n",
        "print(\"üìÇ Loading best model from Phase 2...\")\n",
        "best_model = keras.models.load_model(\n",
        "    'models/best_model_phase2.h5',\n",
        "    custom_objects={'r2_keras': r2_keras}\n",
        ")\n",
        "print(\"‚úÖ Best model loaded successfully\")\n",
        "\n",
        "# Evaluate on validation set\n",
        "print(\"\\nüìä Evaluating model on validation set...\\n\")\n",
        "val_results = best_model.evaluate(val_generator, verbose=1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Loss (MSE):           {val_results[0]:.4f}\")\n",
        "print(f\"Mean Absolute Error:  {val_results[1]:.4f} kg/ha\")\n",
        "print(f\"Mean Squared Error:   {val_results[2]:.4f}\")\n",
        "print(f\"R¬≤ Score:             {val_results[3]:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on validation set\n",
        "print(\"\\nüîÆ Generating predictions on validation set...\")\n",
        "val_predictions = best_model.predict(val_generator, verbose=1)\n",
        "\n",
        "# Get actual values\n",
        "val_actual = val_data['biomass'].values\n",
        "\n",
        "# Flatten predictions\n",
        "val_predictions = val_predictions.flatten()\n",
        "\n",
        "# Calculate additional metrics\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "mae = mean_absolute_error(val_actual, val_predictions)\n",
        "mse = mean_squared_error(val_actual, val_predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(val_actual, val_predictions)\n",
        "mape = mean_absolute_percentage_error(val_actual, val_predictions) * 100\n",
        "\n",
        "print(\"\\
